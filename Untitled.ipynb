{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1260eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8f74de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch2/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for ccdv/arxiv-summarization contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ccdv/arxiv-summarization\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset=datasets.load_dataset(\"ccdv/arxiv-summarization\",split='train',streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78251d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset=list(dataset.take(3500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516435f",
   "metadata": {},
   "source": [
    "# BATCH SIZE: 4 (papers)\n",
    "# CHUNK SIZE: 5 (each paper broken into 5 chunks of n tokens each)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#        forward pass 1 | FP 2    | FP 3    | FP 4    | FP 5    |\n",
    "#\n",
    "# paper 1:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 2:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 3:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 4:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "#\n",
    "#\n",
    "#\n",
    "#        forward pass 6 | FP 7    | FP 8    | FP 9    | FP 10   |\n",
    "#\n",
    "# paper 5:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 6:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 7:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 8:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba073340",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments =10\n",
    "segment_lenght=512\n",
    "\n",
    "chunk_size = segments * segment_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f476f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565d1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles=[x['article'] for x in raw_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d03bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles= [x for x  in raw_articles if len(x)>5120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba04000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles 3401\n"
     ]
    }
   ],
   "source": [
    "print(\"number of articles\", len(raw_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aceaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(tokens):\n",
    "    return ''.join([chr(i) for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e104d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_text(np.frombuffer(raw_articles[0],dtype=np.uint8)[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023661df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character set length 70\n",
      "character set \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "unique_chars=set(''.join([i for i in raw_articles]))\n",
    "print(\"character set length\", len(unique_chars))\n",
    "print(\"character set\", ''.join(sorted(unique_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8156160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5308/1274892679.py:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  converted=[np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]\n"
     ]
    }
   ],
   "source": [
    "converted=[np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8acb2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_article(doc, chunk_size):\n",
    "    remainder=len(doc)%chunk_size\n",
    "    return doc[:-remainder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ce3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped=[clip_article(doc, 5120) for doc in converted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56a06b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped[2].shape[0]/5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9c6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = set([doc.shape for doc in clipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1dd9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5120,),\n",
       " (10240,),\n",
       " (15360,),\n",
       " (20480,),\n",
       " (25600,),\n",
       " (30720,),\n",
       " (35840,),\n",
       " (40960,),\n",
       " (46080,),\n",
       " (51200,),\n",
       " (56320,),\n",
       " (61440,),\n",
       " (66560,),\n",
       " (71680,),\n",
       " (76800,),\n",
       " (81920,),\n",
       " (87040,),\n",
       " (92160,),\n",
       " (97280,),\n",
       " (102400,),\n",
       " (107520,),\n",
       " (112640,),\n",
       " (117760,),\n",
       " (122880,),\n",
       " (128000,),\n",
       " (133120,),\n",
       " (138240,),\n",
       " (143360,),\n",
       " (148480,),\n",
       " (153600,),\n",
       " (158720,),\n",
       " (163840,),\n",
       " (184320,),\n",
       " (189440,),\n",
       " (194560,),\n",
       " (204800,),\n",
       " (209920,),\n",
       " (220160,),\n",
       " (225280,),\n",
       " (230400,),\n",
       " (245760,)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb3e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked=[doc.reshape(-1,chunk_size) for doc in clipped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2145b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5b4b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data=torch.tensor(np.concatenate(chunked),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b69a458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20853, 5120])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c88a12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=iter(DataLoader(processed_data, batch_size=8, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "961d613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc200a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5120])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3a74cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, labels=example[:,:-1],example[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3446cbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([110,  32, 102, 111, 108, 108, 111, 119, 115,  32, 115, 105, 109, 105,\n",
       "        108])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "266ad30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5119])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a316868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n follows similarly to ( [ floctypeq ] ) . \n",
      " + the statement follows similarly when @xmath182 holds by @xmath231 . \n",
      " ( derivation by @xmath238 ) we have then that @xmath240 and thus @xmath311 . \n",
      " [ lem : termintromonotone ] if @xmath207 and @xmath74 then @xmath328 . \n",
      " let @xmath207 and @xmath74 . from the definition we have that @xmath182 . \n",
      " from lemma [ lem : typeintromonotone ] we have that @xmath293 . by induction on the derivation of @xmath182 .    1 . \n",
      " let @xmath182 by @xmath218 . \n",
      " by induction on t \n",
      "**********\n",
      "\n",
      "he derivation of @xmath207 \n",
      " let @xmath329 . \n",
      " then @xmath330 and @xmath328 . \n",
      " 2 .   let @xmath205{\\!:\\!}a , k\\notin{\\textup{dom(}p\\textup{)}}$ ] and @xmath331 . if @xmath150 then @xmath332 , since the derivation of @xmath333 is strictly smaller than the derivation @xmath207 , by ih , @xmath328 . otherwise , @xmath334 and @xmath335 { \\!:\\!}a$ ] . \n",
      " but @xmath336 and by ih @xmath337 . by the definition @xmath328 . \n",
      " + the statement follows similarly when @xmath182 is derived by @xmath201 or @xmath213 . \n",
      " 2  \n",
      "**********\n",
      "\n",
      ".   let @xmath182 by @xmath238 . \n",
      " the statement follows similarly to lemma  [ lem : typeintromonotone ] . \n",
      " 3 .   let @xmath182 by @xmath222 and let @xmath223 . from @xmath338)$ ] and \n",
      " @xmath339)$ ] we have directly that @xmath340)$ ] and @xmath341)$ ] . thus @xmath328 . \n",
      " 4 .   let @xmath182 by @xmath231 and let @xmath232 . \n",
      " by induction on the derivation of @xmath207 . from @xmath234 and \n",
      " @xmath235 $ ] , by ih , @xmath342 and @xmath343 $ ] , thus @xmath328 . \n",
      " let @xmath182 by @xmath266 and let @xmath \n",
      "**********\n",
      "\n",
      "267 , k\\notin { \\textup{dom(}p\\textup{)}}$ ] and @xmath268 . \n",
      " by induction on the derivation of @xmath207 . \n",
      " if @xmath314 then @xmath315 from the premise @xmath331 , by ih , @xmath328 \n",
      " . otherwise , @xmath334 and @xmath319 $ ] ( i.e.  @xmath293 by @xmath266 ) . since @xmath344 , by the ih , @xmath337 . by the definition @xmath328 . \n",
      " [ lem : termeqmonotone ] if @xmath345 and @xmath74 then @xmath346 . \n",
      " let @xmath209 and @xmath74 . \n",
      " we have then that @xmath182 , @xmath207 , and @xmath208 . by lemma  [ le \n",
      "**********\n",
      "\n",
      "m : typeintromonotone ] @xmath293 . by lemma  [ lem : termintromonotone ] @xmath347 and @xmath348 . by induction on the derivation @xmath182 .    1 . \n",
      " let @xmath182 by @xmath218 . \n",
      " by induction on the derivation of @xmath345 \n",
      " let @xmath329 and @xmath349 . \n",
      " by monotonicity of reduction @xmath346 . \n",
      " [ localtermeqforcingismonotone ] let @xmath329 and @xmath205{\\!:\\!}a , k\\notin{\\textup{dom(}p\\textup{)}}$ ] and @xmath350 . \n",
      " if @xmath150 then @xmath332 , by ih , @xmath346 . otherwise , @xmath335{\\!:\\!}a$ ] \n",
      "**********\n",
      "\n",
      " . but @xmath336 and by ih @xmath351 . by the definition @xmath346 . \n",
      " 3 .   let @xmath205{\\!:\\!}a , k\\notin{\\textup{dom(}p\\textup{)}}$ ] . \n",
      " the statement follows similarly to ( [ localtermeqforcingismonotone ] ) . \n",
      " + the statement follows similarly for when @xmath182 holds by @xmath201 or @xmath213 . \n",
      " 2 .   let @xmath182 by @xmath238 . \n",
      " the statement follows by a proof similar to that of lemma  [ lem : typeeqmonotone ] . \n",
      " 3 .   let @xmath182 by @xmath222 and let @xmath223 . from @xmath352)$ \n",
      " ] we hav \n",
      "**********\n",
      "\n",
      "e directly that @xmath353)$ ] . \n",
      " hence @xmath346 . \n",
      " 4 .   let @xmath182 by @xmath231 and let @xmath232 . \n",
      " by induction on the derivation of @xmath345 . from @xmath354 and \n",
      " @xmath355 $ ] , by ih we have @xmath356 and @xmath357 $ ] , thus @xmath346 . 5 . \n",
      " let @xmath182 by @xmath266 and let @xmath267 , k \\notin{\\textup{dom(}p\\textup{)}}$ ] . \n",
      " by induction on the derivation of @xmath209 . if @xmath150 then the statement follows by ih . if @xmath334 then @xmath319 $ ] ( i.e.  @xmath293 by @xmath266 ) and s \n",
      "**********\n",
      "\n",
      "ince @xmath358 , by ih , @xmath359 . \n",
      " hence @xmath346 . \n",
      " we collect the results of lemmas  [ lem : typeintromonotone ] , [ lem : termintromonotone ] , [ lem : termeqmonotone ] , and [ lem : typeeqmonotone ] in the following corollary . \n",
      " [ cor : monotonicity ] if @xmath190 and @xmath74 then @xmath191 . \n",
      " we write @xmath360 when @xmath361 . by monotonicity \n",
      " @xmath360 iff @xmath190 for all @xmath40 . \n",
      " [ lem : localitysublemma1 ] let @xmath182 and @xmath194 . if @xmath362 and @xmath363 then @xmath185 .     \n",
      "**********\n",
      "\n",
      "by induction on the derivation of @xmath182 .    1 . \n",
      " let @xmath182 by @xmath218 . \n",
      " by induction on the derivation of @xmath194 1 .   if @xmath194 by @xmath218 then @xmath185 immediately . \n",
      " [ typeqrightloc ] if @xmath194 by @xmath266 . \n",
      " the statement follows similarly to  ( [ loctypeqloc ] ) below . \n",
      " + the statement follows similarly when @xmath182 is derived by @xmath364 and @xmath213 . \n",
      " 2 .   let @xmath182 by @xmath222 and let @xmath223 . \n",
      " by induction on the derivation of @xmath194 1 . \n",
      " let @xmat \n",
      "**********\n",
      "\n",
      "h194 by @xmath222 and let @xmath225 . since @xmath182 and @xmath194 we have @xmath298 and @xmath365 . \n",
      " from the premise @xmath366 and by ih @xmath226 . \n",
      " + let @xmath74 and @xmath367 . if @xmath368 then @xmath369=e[a]$ ] and @xmath185 . \n",
      " otherwise , since @xmath309 , by monotonicity @xmath370=e[a]$ ] . \n",
      " from @xmath182 we have that @xmath369 $ ] and from @xmath194 we have that @xmath371 $ ] . by ih @xmath369 = e[a]$ ] . \n",
      " we thus have @xmath185 . \n",
      " 2 .   let @xmath194 by @xmath266 . \n",
      " the statement then  \n",
      "**********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_segment, labels_segment in zip(seq.chunk(10,dim=-1),labels.chunk(10,dim=-1)):\n",
    "    print(decode_text(seq_segment[0]),\"\\n**********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f724e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(\n",
    "    nn.Embedding(128,16), #(vocab_size, embedding_dim)\n",
    "    nn.Linear(16,150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150,150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150,128), #(params, vocab_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862e21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceea121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1e61aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(128, 16)\n",
       "  (1): Linear(in_features=16, out_features=150, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=150, out_features=150, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=150, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21632e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d8a5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55968279838562\n",
      "3.3730733394622803\n",
      "3.046580100059509\n",
      "2.939957523345947\n",
      "2.9663415431976317\n",
      "2.83984375\n",
      "2.8321441888809202\n",
      "2.7357513666152955\n",
      "2.6825407981872558\n",
      "2.609228801727295\n",
      "2.621779370307922\n",
      "2.7563969612121584\n",
      "2.635390543937683\n",
      "2.5794907808303833\n",
      "2.5554212808609007\n",
      "2.5501531600952148\n",
      "2.5392120122909545\n",
      "2.481417107582092\n",
      "2.474931073188782\n",
      "2.4647560596466063\n",
      "2.477619981765747\n",
      "2.4739154100418093\n",
      "2.5278233528137206\n",
      "2.4088573932647703\n",
      "2.473329019546509\n",
      "2.450107789039612\n",
      "2.5675188064575196\n",
      "2.6088327407836913\n",
      "2.454421043395996\n",
      "2.4861522197723387\n",
      "2.4466771125793456\n",
      "2.455324983596802\n",
      "2.491788864135742\n",
      "2.481079912185669\n",
      "2.4183900594711303\n",
      "2.4627229452133177\n",
      "2.602856183052063\n",
      "2.53012101650238\n",
      "2.4814670085906982\n",
      "2.4994054794311524\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    \n",
    "    data=next(loader) #(batch_size, sequence_length) #(8,5120)\n",
    "    seq,labels= data[:,:-1], data[:,1:]\n",
    "    train_loss=0.\n",
    "    \n",
    "    for seq_segment, labels_segment in zip(seq.chunk(segments,dim=-1),labels.chunk(segments, dim=-1)):#ten passes of (8, 512)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=model(seq_segment)\n",
    "        #print(y_pred.shape)\n",
    "        y_pred=y_pred.transpose(2,1)\n",
    "        loss=loss_fn(y_pred,labels_segment)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss +=loss.item()\n",
    "        \n",
    "        \n",
    "    if i%5==0:\n",
    "        print(train_loss/segments)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a48ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
